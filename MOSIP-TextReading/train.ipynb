{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7337eee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Generic Handwriting OCR with Text Detection + TrOCR\n",
      "Using device: cuda\n",
      "Loading handwritten OCR model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TrOCR model loaded\n",
      "Loading text detection model...\n",
      "‚úÖ Text detection model loaded\n",
      "üì∏ Found image: c:\\Users\\shaiv\\OneDrive - iiit-b\\Desktop\\IIITB\\Projects\\MOSIP Hackathon\\MOSIP-TextReading\\Images\\Handwriting\\image.png\n",
      "\n",
      "üîç Processing: image.png\n",
      "üîç Detected 35 text regions\n",
      "Processing text_region_0... 'first'\n",
      "Processing text_region_1... 'manne'\n",
      "Processing text_region_2... 'abigail'\n",
      "Processing text_region_3... 'midde'\n",
      "Processing text_region_4... 'norme'\n",
      "Processing text_region_5... 'resource'\n",
      "Processing text_region_6... 'last'\n",
      "Processing text_region_7... 'name'\n",
      "Processing text_region_8... 'summer'\n",
      "Processing text_region_9... 'creander'\n",
      "Processing text_region_10... 'Female'\n",
      "Processing text_region_11... 'Date of'\n",
      "Processing text_region_12... 'birth'\n",
      "Processing text_region_13... '27-019-2000'\n",
      "Processing text_region_14... 'address'\n",
      "Processing text_region_15... 'Line 1'\n",
      "Processing text_region_16... 'read H'\n",
      "Processing text_region_17... 'street H2'\n",
      "Processing text_region_18... 'address'\n",
      "Processing text_region_19... 'Line 2'\n",
      "Processing text_region_20... 'hsr'\n",
      "Processing text_region_21... 'layout'\n",
      "Processing text_region_22... 'city'\n",
      "Processing text_region_23... 'Bangalore'\n",
      "Processing text_region_24... 'state'\n",
      "Processing text_region_25... 'karnataka'\n",
      "Processing text_region_26... 'plin code'\n",
      "Processing text_region_27... '5600068'\n",
      "Processing text_region_28... 'phone'\n",
      "Processing text_region_29... 'mumbers'\n",
      "Processing text_region_30... '9987659110'\n",
      "Processing text_region_31... 'Email ID'\n",
      "Processing text_region_32... 'abigailo'\n",
      "Processing text_region_33... 'comi'\n",
      "Processing text_region_34... 'general'\n",
      "üîç Debug image saved: debug_detection.png\n",
      "üè∑Ô∏è  Found 21 potential labels and 14 potential values\n",
      "‚úÖ Paired: 'First Name' -> 'abigail'\n",
      "‚úÖ Paired: 'Middle Name' -> 'resource'\n",
      "‚úÖ Paired: 'Last Name' -> 'summer'\n",
      "‚úÖ Paired: 'Creander' -> 'Female'\n",
      "‚úÖ Paired: 'Date of Birth' -> '27-019-2000'\n",
      "‚úÖ Paired: 'Date of Birth' -> 'read H'\n",
      "‚úÖ Paired: 'Address Line 1' -> 'street H2'\n",
      "‚úÖ Paired: 'Address Line 2' -> 'hsr'\n",
      "‚úÖ Paired: 'Bangalore' -> '5600068'\n",
      "‚úÖ Paired: 'Pin Code' -> '9987659110'\n",
      "‚úÖ Paired: 'Phone Number' -> 'abigailo'\n",
      "‚úÖ Paired: 'Email ID' -> 'general'\n",
      "\n",
      "üìã EXTRACTED FORM FIELDS:\n",
      "==================================================\n",
      "First Name: abigail\n",
      "Middle Name: resource\n",
      "Last Name: summer\n",
      "Creander: Female\n",
      "Date of Birth: read H\n",
      "Address Line 1: street H2\n",
      "Address Line 2: hsr\n",
      "Bangalore: 5600068\n",
      "Pin Code: 9987659110\n",
      "Phone Number: abigailo\n",
      "Email ID: general\n",
      "==================================================\n",
      "‚úÖ Successfully processed 35 text regions\n",
      "üìÅ Results saved to: output.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, TrOCRProcessor\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageEnhance, ImageFilter\n",
    "import numpy as np\n",
    "import cv2\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from transformers import pipeline\n",
    "import easyocr\n",
    "\n",
    "class GenericHandwritingOCR:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        self.load_models()\n",
    "    \n",
    "    def load_models(self):\n",
    "        \"\"\"Load text detection and handwritten OCR models\"\"\"\n",
    "        try:\n",
    "            print(\"Loading handwritten OCR model...\")\n",
    "            self.processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\n",
    "            self.model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten').to(self.device)\n",
    "            print(\"‚úÖ TrOCR model loaded\")\n",
    "            \n",
    "            print(\"Loading text detection model...\")\n",
    "            self.text_detector = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
    "            print(\"‚úÖ Text detection model loaded\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Model loading failed: {e}\")\n",
    "            self.model = None\n",
    "            self.text_detector = None\n",
    "    \n",
    "    def detect_text_regions(self, image_path: str) -> List[Dict]:\n",
    "        \"\"\"Use Hugging Face model to detect text locations in the image\"\"\"\n",
    "        if self.text_detector is None:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Read image\n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            # Use EasyOCR to detect text regions\n",
    "            results = self.text_detector.readtext(image_path, detail=1, paragraph=False)\n",
    "            \n",
    "            text_regions = []\n",
    "            for i, (bbox, text, confidence) in enumerate(results):\n",
    "                # Convert bbox to standard format (x1, y1, x2, y2)\n",
    "                bbox = np.array(bbox)\n",
    "                x1, y1 = bbox.min(axis=0).astype(int)\n",
    "                x2, y2 = bbox.max(axis=0).astype(int)\n",
    "                \n",
    "                # Add padding for better OCR\n",
    "                padding = 10\n",
    "                x1 = max(0, x1 - padding)\n",
    "                y1 = max(0, y1 - padding)\n",
    "                x2 = min(image.shape[1], x2 + padding)\n",
    "                y2 = min(image.shape[0], y2 + padding)\n",
    "                \n",
    "                text_regions.append({\n",
    "                    'id': f'text_region_{i}',\n",
    "                    'bbox': (x1, y1, x2, y2),\n",
    "                    'detected_text': text,\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "            \n",
    "            print(f\"üîç Detected {len(text_regions)} text regions\")\n",
    "            return text_regions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Text detection failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def advanced_preprocessing(self, image: Image.Image) -> List[Image.Image]:\n",
    "        \"\"\"Apply multiple preprocessing techniques for better handwriting recognition\"\"\"\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image)\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        if image.mode != 'L':\n",
    "            image = image.convert('L')\n",
    "        \n",
    "        variants = []\n",
    "        img_array = np.array(image)\n",
    "        \n",
    "        # Variant 1: Enhanced CLAHE\n",
    "        clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(4,4))\n",
    "        clahe_img = clahe.apply(img_array)\n",
    "        clahe_enhanced = cv2.convertScaleAbs(clahe_img, alpha=1.3, beta=10)\n",
    "        variants.append(Image.fromarray(clahe_enhanced).convert('RGB'))\n",
    "        \n",
    "        # Variant 2: Denoising + contrast\n",
    "        denoised = cv2.fastNlMeansDenoising(img_array, h=10)\n",
    "        enhanced = cv2.convertScaleAbs(denoised, alpha=1.8, beta=30)\n",
    "        variants.append(Image.fromarray(enhanced).convert('RGB'))\n",
    "        \n",
    "        # Variant 3: Morphological preprocessing\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,1))\n",
    "        morph = cv2.morphologyEx(img_array, cv2.MORPH_CLOSE, kernel)\n",
    "        sharp_kernel = np.array([[-1,-1,-1,-1,-1], \n",
    "                                [-1,2,2,2,-1], \n",
    "                                [-1,2,8,2,-1], \n",
    "                                [-1,2,2,2,-1], \n",
    "                                [-1,-1,-1,-1,-1]]) / 8.0\n",
    "        sharpened = cv2.filter2D(morph, -1, sharp_kernel)\n",
    "        variants.append(Image.fromarray(np.clip(sharpened, 0, 255).astype(np.uint8)).convert('RGB'))\n",
    "        \n",
    "        return variants\n",
    "    \n",
    "    def recognize_handwriting_with_trocr(self, region_image: Image.Image) -> str:\n",
    "        \"\"\"Use TrOCR to recognize handwritten text in the region\"\"\"\n",
    "        if self.model is None:\n",
    "            return \"\"\n",
    "        \n",
    "        variants = self.advanced_preprocessing(region_image)\n",
    "        results = []\n",
    "        \n",
    "        for i, variant in enumerate(variants):\n",
    "            try:\n",
    "                pixel_values = self.processor(images=variant, return_tensors=\"pt\").pixel_values.to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    generated_ids = self.model.generate(\n",
    "                        pixel_values,\n",
    "                        max_length=50,\n",
    "                        num_beams=6,\n",
    "                        early_stopping=True,\n",
    "                        do_sample=False,\n",
    "                        no_repeat_ngram_size=3,\n",
    "                        length_penalty=1.0,\n",
    "                        repetition_penalty=1.1\n",
    "                    )\n",
    "                \n",
    "                text = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "                if text and len(text) > 0:\n",
    "                    results.append((text, i))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"TrOCR attempt {i} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not results:\n",
    "            return \"\"\n",
    "        \n",
    "        # Return the best result\n",
    "        best_result = max(results, key=lambda x: len(x[0]) if x[0] else 0)\n",
    "        return best_result[0]\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Enhanced text cleaning for values\"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return \"\"\n",
    "        \n",
    "        # Basic cleaning\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Multiple spaces to single space\n",
    "        \n",
    "        # Remove trailing periods that are common OCR artifacts\n",
    "        text = re.sub(r'\\s*\\.\\s*$', '', text)\n",
    "        \n",
    "        # Clean up common OCR errors for specific patterns\n",
    "        # Phone numbers\n",
    "        if re.match(r'[\\d\\-\\s]+', text):\n",
    "            text = re.sub(r'[^\\d\\-]', '', text)\n",
    "        \n",
    "        # Email addresses\n",
    "        if '@' in text:\n",
    "            text = re.sub(r'[^\\w@.-]', '', text)\n",
    "        \n",
    "        # Dates\n",
    "        if re.match(r'[\\d\\-/\\s]+', text):\n",
    "            text = re.sub(r'[^\\d\\-/]', '', text)\n",
    "        \n",
    "        # General cleanup - keep alphanumeric, spaces, and common punctuation\n",
    "        text = re.sub(r'[^\\w\\s@.-]', '', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def create_debug_visualization(self, image_path: str, regions: List[Dict]):\n",
    "        \"\"\"Create debug image showing detected regions\"\"\"\n",
    "        image = Image.open(image_path)\n",
    "        debug_image = image.copy()\n",
    "        draw = ImageDraw.Draw(debug_image)\n",
    "        \n",
    "        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', \n",
    "                 'pink', 'gray', 'olive', 'navy', 'cyan', 'magenta']\n",
    "        \n",
    "        for i, region in enumerate(regions):\n",
    "            color = colors[i % len(colors)]\n",
    "            x1, y1, x2, y2 = region['bbox']\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=color, width=2)\n",
    "            region_id = region.get('id', f'region_{i}')\n",
    "            draw.text((x1, y1-15), region_id, fill=color)\n",
    "        \n",
    "        debug_path = 'debug_detection.png'\n",
    "        debug_image.save(debug_path)\n",
    "        print(f\"üîç Debug image saved: {debug_path}\")\n",
    "    \n",
    "    def pair_labels_with_values(self, regions: List[Dict]) -> Dict[str, str]:\n",
    "        \"\"\"Pair label regions with their corresponding value regions based on spatial proximity\"\"\"\n",
    "        \n",
    "        # Sort regions by position (top to bottom, left to right)\n",
    "        sorted_regions = sorted(regions, key=lambda x: (x['position']['y'], x['position']['x']))\n",
    "        \n",
    "        key_value_pairs = {}\n",
    "        used_regions = set()\n",
    "        \n",
    "        # First pass: identify potential labels and values\n",
    "        labels = []\n",
    "        values = []\n",
    "        \n",
    "        for i, region in enumerate(sorted_regions):\n",
    "            text = region['text'].lower().strip()\n",
    "            \n",
    "            # Enhanced label detection\n",
    "            label_keywords = ['name', 'first', 'middle', 'last', 'gender', 'date', 'birth', \n",
    "                            'address', 'line', 'city', 'state', 'phone', 'email', 'code', 'pin', 'plin']\n",
    "            \n",
    "            # Check if this region looks like a label\n",
    "            is_label = any(keyword in text for keyword in label_keywords)\n",
    "            \n",
    "            # Additional checks for labels\n",
    "            if not is_label:\n",
    "                # Check if it's a short text that could be a label\n",
    "                if len(text.split()) <= 3 and any(char.isalpha() for char in text):\n",
    "                    # Check if it's positioned on the left side of the image\n",
    "                    if region['position']['x'] < 200:  # Assuming labels are on the left\n",
    "                        is_label = True\n",
    "            \n",
    "            if is_label:\n",
    "                labels.append((i, region))\n",
    "            else:\n",
    "                values.append((i, region))\n",
    "        \n",
    "        print(f\"üè∑Ô∏è  Found {len(labels)} potential labels and {len(values)} potential values\")\n",
    "        \n",
    "        # Second pass: pair labels with values\n",
    "        for label_idx, label_region in labels:\n",
    "            if label_idx in used_regions:\n",
    "                continue\n",
    "            \n",
    "            label_text = label_region['text'].lower().strip()\n",
    "            label_center_x = label_region['position']['x'] + label_region['position']['width'] / 2\n",
    "            label_center_y = label_region['position']['y'] + label_region['position']['height'] / 2\n",
    "            label_right = label_region['position']['x'] + label_region['position']['width']\n",
    "            \n",
    "            best_value = None\n",
    "            best_value_idx = -1\n",
    "            min_distance = float('inf')\n",
    "            \n",
    "            for value_idx, value_region in values:\n",
    "                if value_idx in used_regions:\n",
    "                    continue\n",
    "                \n",
    "                value_text = value_region['text'].lower().strip()\n",
    "                value_center_x = value_region['position']['x'] + value_region['position']['width'] / 2\n",
    "                value_center_y = value_region['position']['y'] + value_region['position']['height'] / 2\n",
    "                value_left = value_region['position']['x']\n",
    "                \n",
    "                # Skip if value also looks like a label\n",
    "                if any(keyword in value_text for keyword in ['name', 'first', 'middle', 'last', 'date', 'birth', 'address', 'line', 'city', 'state', 'phone', 'email']):\n",
    "                    continue\n",
    "                \n",
    "                # Calculate distances\n",
    "                horizontal_distance = abs(value_center_x - label_center_x)\n",
    "                vertical_distance = abs(value_center_y - label_center_y)\n",
    "                \n",
    "                # Prefer values that are:\n",
    "                # 1. On the same row (small vertical distance)\n",
    "                # 2. To the right of the label\n",
    "                # 3. Close horizontally\n",
    "                \n",
    "                is_same_row = vertical_distance < 50  # Allow some tolerance for same row\n",
    "                is_to_the_right = value_left > label_right - 50  # Value should be to the right\n",
    "                \n",
    "                if is_same_row and is_to_the_right:\n",
    "                    # Same row pairing - prioritize horizontal distance\n",
    "                    distance = horizontal_distance + vertical_distance * 0.1\n",
    "                elif vertical_distance < 150:  # Different row but close vertically\n",
    "                    # Check if they're roughly aligned (label above value)\n",
    "                    horizontal_alignment = abs(label_center_x - value_center_x)\n",
    "                    if horizontal_alignment < 100:  # Reasonably aligned\n",
    "                        distance = vertical_distance + horizontal_alignment * 0.5\n",
    "                    else:\n",
    "                        continue  # Skip if not aligned\n",
    "                else:\n",
    "                    continue  # Too far apart\n",
    "                \n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    best_value = value_region\n",
    "                    best_value_idx = value_idx\n",
    "            \n",
    "            # Pair the label with the best value found\n",
    "            if best_value and min_distance < 300:  # Reasonable maximum distance\n",
    "                clean_label = self.format_label(label_region['text'])\n",
    "                clean_value = self.clean_text(best_value['text'])\n",
    "                \n",
    "                if clean_label and clean_value:\n",
    "                    key_value_pairs[clean_label] = clean_value\n",
    "                    used_regions.add(label_idx)\n",
    "                    used_regions.add(best_value_idx)\n",
    "                    print(f\"‚úÖ Paired: '{clean_label}' -> '{clean_value}'\")\n",
    "        \n",
    "        return key_value_pairs\n",
    "    \n",
    "    def format_label(self, text: str) -> str:\n",
    "        \"\"\"Format label text into proper field names\"\"\"\n",
    "        text = text.lower().strip()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        \n",
    "        label_mappings = {\n",
    "            'first name': 'First Name',\n",
    "            'first': 'First Name',\n",
    "            'middle name': 'Middle Name', \n",
    "            'middle': 'Middle Name',\n",
    "            'midde': 'Middle Name',  # Common OCR error\n",
    "            'manne': 'Middle Name',  # Common OCR error\n",
    "            'last name': 'Last Name',\n",
    "            'last': 'Last Name',\n",
    "            'date of birth': 'Date of Birth',\n",
    "            'birth': 'Date of Birth',\n",
    "            'date': 'Date of Birth',\n",
    "            'address line 1': 'Address Line 1',\n",
    "            'line 1': 'Address Line 1',\n",
    "            'address': 'Address Line 1',\n",
    "            'address line 2': 'Address Line 2', \n",
    "            'line 2': 'Address Line 2',\n",
    "            'city': 'City',\n",
    "            'state': 'State',\n",
    "            'pin code': 'Pin Code',\n",
    "            'plin code': 'Pin Code',  # Common OCR error\n",
    "            'phone number': 'Phone Number',\n",
    "            'phone': 'Phone Number',\n",
    "            'mumbers': 'Phone Number',  # Common OCR error\n",
    "            'email id': 'Email ID',\n",
    "            'email': 'Email ID',\n",
    "            'gender': 'Gender'\n",
    "        }\n",
    "        \n",
    "        # Try exact match first\n",
    "        if text in label_mappings:\n",
    "            return label_mappings[text]\n",
    "        \n",
    "        # Try partial matches for multi-word labels\n",
    "        for key, value in label_mappings.items():\n",
    "            if len(key.split()) > 1:  # Multi-word labels\n",
    "                key_words = key.split()\n",
    "                text_words = text.split()\n",
    "                if len(set(key_words) & set(text_words)) >= len(key_words) - 1:  # Allow one word difference\n",
    "                    return value\n",
    "        \n",
    "        # Try single word matches\n",
    "        for key, value in label_mappings.items():\n",
    "            if key in text or text in key:\n",
    "                return value\n",
    "        \n",
    "        # Fallback: capitalize each word\n",
    "        return ' '.join(word.capitalize() for word in text.split())\n",
    "\n",
    "    def process_image(self, image_path: str) -> Dict:\n",
    "        \"\"\"Main processing function: detect text regions and recognize handwriting\"\"\"\n",
    "        print(f\"\\nüîç Processing: {os.path.basename(image_path)}\")\n",
    "        \n",
    "        if self.model is None or self.text_detector is None:\n",
    "            return {'error': 'Models not loaded'}\n",
    "        \n",
    "        # Step 1: Detect text regions using Hugging Face model\n",
    "        text_regions = self.detect_text_regions(image_path)\n",
    "        \n",
    "        if not text_regions:\n",
    "            return {'error': 'No text regions detected'}\n",
    "        \n",
    "        # Step 2: Process each detected region with TrOCR\n",
    "        image = Image.open(image_path)\n",
    "        processed_regions = []\n",
    "        \n",
    "        for region_info in text_regions:\n",
    "            region_id = region_info['id']\n",
    "            x1, y1, x2, y2 = region_info['bbox']\n",
    "            \n",
    "            print(f\"Processing {region_id}...\", end=\" \")\n",
    "            \n",
    "            # Extract region from image\n",
    "            region_image = image.crop((x1, y1, x2, y2))\n",
    "            \n",
    "            # Recognize handwriting with TrOCR\n",
    "            recognized_text = self.recognize_handwriting_with_trocr(region_image)\n",
    "            \n",
    "            # Clean the text (basic cleaning only)\n",
    "            if recognized_text:\n",
    "                cleaned_text = self.clean_text(recognized_text)\n",
    "                if cleaned_text:\n",
    "                    processed_regions.append({\n",
    "                        'id': region_id,\n",
    "                        'text': cleaned_text,\n",
    "                        'bbox': [int(x1), int(y1), int(x2), int(y2)],\n",
    "                        'confidence': float(region_info.get('confidence', 0.0)),\n",
    "                        'position': {\n",
    "                            'x': int(x1),\n",
    "                            'y': int(y1),\n",
    "                            'width': int(x2 - x1),\n",
    "                            'height': int(y2 - y1)\n",
    "                        }\n",
    "                    })\n",
    "                    print(f\"'{cleaned_text}'\")\n",
    "                else:\n",
    "                    print(\"‚ùå (empty after cleaning)\")\n",
    "            else:\n",
    "                print(\"‚ùå (no text recognized)\")\n",
    "        \n",
    "        # Create debug visualization\n",
    "        self.create_debug_visualization(image_path, processed_regions)\n",
    "        \n",
    "        key_value_pairs = self.pair_labels_with_values(processed_regions)\n",
    "        \n",
    "        output_data = {\n",
    "            'image_path': image_path,\n",
    "            'total_regions': len(processed_regions),\n",
    "            'form_fields': key_value_pairs,  # Changed from 'extracted_text' to 'form_fields'\n",
    "            'raw_regions': {}  # Added raw regions for debugging\n",
    "        }\n",
    "        \n",
    "        # Also include raw regions for debugging\n",
    "        sorted_regions = sorted(processed_regions, key=lambda x: (x['position']['y'], x['position']['x']))\n",
    "        \n",
    "        for i, region in enumerate(sorted_regions):\n",
    "            key = f\"text_field_{i+1}\"\n",
    "            output_data['raw_regions'][key] = {\n",
    "                'value': region['text'],\n",
    "                'confidence': region['confidence'],\n",
    "                'bbox': region['bbox'],\n",
    "                'position': region['position']\n",
    "            }\n",
    "        \n",
    "        return output_data\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process handwritten text and output to JSON\"\"\"\n",
    "    print(\"üéØ Generic Handwriting OCR with Text Detection + TrOCR\")\n",
    "    \n",
    "    ocr = GenericHandwritingOCR()\n",
    "    \n",
    "    if ocr.model is None or ocr.text_detector is None:\n",
    "        print(\"‚ùå Cannot proceed without models\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # Running in Jupyter notebook or interactive environment\n",
    "        script_dir = os.getcwd()\n",
    "    \n",
    "    possible_paths = [\n",
    "        os.path.join(script_dir, \"Images\", \"Handwriting\", \"image.png\"),\n",
    "        os.path.join(script_dir, \"images\", \"handwriting\", \"image.png\"),\n",
    "        os.path.join(script_dir, \"image.png\"),\n",
    "        \"Images/Handwriting/image.png\",\n",
    "        \"images/handwriting/image.png\",\n",
    "        \"image.png\"\n",
    "    ]\n",
    "    \n",
    "    image_path = None\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            image_path = path\n",
    "            break\n",
    "    \n",
    "    if not image_path:\n",
    "        print(f\"‚ùå Image not found in any of these locations:\")\n",
    "        for path in possible_paths:\n",
    "            print(f\"   {path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üì∏ Found image: {image_path}\")\n",
    "    \n",
    "    # Process the image\n",
    "    results = ocr.process_image(image_path)\n",
    "    \n",
    "    if 'error' in results:\n",
    "        print(f\"‚ùå Error: {results['error']}\")\n",
    "        return\n",
    "    \n",
    "    output_file = 'output.json'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nüìã EXTRACTED FORM FIELDS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if results.get('form_fields'):\n",
    "        for field_name, field_value in results['form_fields'].items():\n",
    "            print(f\"{field_name}: {field_value}\")\n",
    "    else:\n",
    "        print(\"No form fields detected\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"‚úÖ Successfully processed {results['total_regions']} text regions\")\n",
    "    print(f\"üìÅ Results saved to: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1650080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
